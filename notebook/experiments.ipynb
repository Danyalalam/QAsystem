{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load Google API Key\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Import necessary modules\n",
    "from llama_index.llms.gemini import Gemini\n",
    "import google.generativeai as genai\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "\n",
    "# Configure Google Generative AI\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize the models\n",
    "gemini_model = Gemini(model_name=\"models/gemini-pro\", api_key=google_api_key)\n",
    "gemini_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "# Load documents using SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(input_dir='../data').load_data()\n",
    "\n",
    "# Build the index with the LLM and embeddings\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    llm=gemini_model,\n",
    "    embed_model=gemini_embed_model,\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=gemini_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Production Grade Machine Learning Project\n",
      "- End TO End Chest Cancer Classification with Mlops(DL)\n",
      "- Transformer From Scratch In PyTorch\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"what project are mentioned\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-1.5-flash-002\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    if 'generateContent' in model.supported_generation_methods:\n",
    "           \n",
    "     print(model.name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = SimpleDirectoryReader('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = document.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Danyal Alam\n",
      "danyalalam514@gmail.com 03175193965 linkedin.com/in/danyal-alam-272593312 \n",
      "github.com/Danyalalam \n",
      "SKILLS\n",
      "Languages — Python, SQL, HTML\n",
      "Frameworks — Tensorflow/Keras, PyTorch, Pandas, NumPy, NLTK\n",
      "MLOps — Git, Docker, MLflow, DVC, BentoML, CI/CD pipelines, GitHub Actions, RESTful APIs\n",
      "Cloud Technologies & Databases — Microsoft Azure, AWS  MySQL , MongoDB\n",
      "PROJECTS\n",
      "Production Grade Machine Learning Project: | MongoDB,AWS,Docker,Git,CI/CD,FastApi\n",
      "Visa Approval Prediction:\n",
      "•End-to-End Pipeline: Built a full machine learning pipeline from data ingestion (via MongoDB) to cloud deployment \n",
      "using Docker and AWS services (EC2, S3, ECR).\n",
      "•Model Management: Trained multiple models, evaluated them using Evidently AI for data drift detection, and pushed \n",
      "the best model to S3 for production.\n",
      "•CI/CD Integration: Implemented automated CI/CD workflows with GitHub Actions and Dockerized FastApi applicatoin \n",
      "for seamless deployment on AWS, ensuring scalability and reliability.\n",
      "End TO End Chest Cancer Classification with Mlops(DL): | CNN,Azure,CI/CD,Docker,Git,Flask\n",
      "•Developed An End-to-End Deep Learning Pipeline : Designed and implemented a modular pipeline for chest cancer \n",
      "detection using CNN, ensuring scalable and maintainable code with enhanced reproducibility.\n",
      "•MLOps Integration for Automated Workflow : Leveraged MLflow and DVC for experiment tracking and version \n",
      "control, and utilized Azure CI/CD pipelines for automated deployment.\n",
      "•Model Deployment with Docker and Flask : Deployed the deep learning model in a Dockerized Flask application, \n",
      "optimizing it for real-world usage and making it accessible via REST API, ensuring rapid and reliable inference.\n",
      "Transformer From Scratch In PyTorch: | PyTorch,Python,NLP\n",
      "•Architecture: Implemented the full transformer architecture, including multi-head attention and feed-forward layers, \n",
      "using PyTorch.\n",
      "•Optimization: Optimized model performance through careful hyperparameter tuning and gradient clipping techniques.\n",
      "•Performance: Achieved comparable results to pre-trained models on NLP tasks such as machine translation or text \n",
      "classification.\n",
      "EDUCATION\n",
      "University Of Engineering And Technology Peshawar\n",
      "• Bachelors of Science  -  Electrical Communication Engineering\n",
      "   GPA 3.0/4.0Sep 2020 – Jul 2024\n",
      "Peshawar, Pakistan\n",
      "EXPERIENCE\n",
      "Machine Learning Intern\n",
      "Digital Empowerment Pakistan\n",
      "•Developed models using scikit-learn for classification and regression tasks. Utilized pandas \n",
      "for data manipulation and Matplotlib for visualization.Aug 2024 – Sep 2024\n",
      "virtual\n",
      "•Applied TF-IDF and Bag-of-Words for feature extraction. Implemented text preprocessing \n",
      "pipelines including tokenization, stemming, and stop word removal using NLTK and spaCy.\n",
      "•Implemented CI/CD pipelines for ML models using GitHub Actions. Containerized \n",
      "applications with Docker and deployed on AWS/Azure. Monitored model performance and \n",
      "data drift using MLflow.\n",
      "CERTIFICATES\n",
      "Complete Machine Learning,NLP Bootcamp MLOPS & Deployment(Udemy):    Learned key concepts in ML, NLP,\n",
      "and advanced topics like deep learning and transformers. Worked on real-world applications using TensorFlow, PyTorch, and\n",
      "scikit-learn. Gained experience in MLOps for model deployment and optimization, focusing on practical workflows and\n",
      "automation.\n",
      "|\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = Gemini(model_name=\"models/gemini-pro\", api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of nodes, objects, or index_struct must be provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_embed_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GenAi\\QAsystem\\venv\\lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:75\u001b[0m, in \u001b[0;36mVectorStoreIndex.__init__\u001b[1;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     69\u001b[0m     resolve_embed_model(embed_model, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m Settings\u001b[38;5;241m.\u001b[39membed_model\n\u001b[0;32m     72\u001b[0m )\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_batch_size \u001b[38;5;241m=\u001b[39m insert_batch_size\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     76\u001b[0m     nodes\u001b[38;5;241m=\u001b[39mnodes,\n\u001b[0;32m     77\u001b[0m     index_struct\u001b[38;5;241m=\u001b[39mindex_struct,\n\u001b[0;32m     78\u001b[0m     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m     79\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39mshow_progress,\n\u001b[0;32m     80\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[0;32m     81\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[0;32m     82\u001b[0m     transformations\u001b[38;5;241m=\u001b[39mtransformations,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     84\u001b[0m )\n",
      "File \u001b[1;32md:\\GenAi\\QAsystem\\venv\\lib\\site-packages\\llama_index\\core\\indices\\base.py:48\u001b[0m, in \u001b[0;36mBaseIndex.__init__\u001b[1;34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize with parameters.\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m objects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of nodes, objects, or index_struct must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly one of nodes or index_struct can be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: One of nodes, objects, or index_struct must be provided."
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(\n",
    "    llm=gemini_model,\n",
    "    embed_model=gemini_embed_model,\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m service_context \u001b[38;5;241m=\u001b[39m \u001b[43mServiceContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43membed_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemini_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\GenAi\\QAsystem\\venv\\lib\\site-packages\\llama_index\\core\\service_context.py:31\u001b[0m, in \u001b[0;36mServiceContext.from_defaults\u001b[1;34m(cls, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_defaults\u001b[39m(\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     24\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceContext\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a ServiceContext from defaults.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m    NOTE: Deprecated, use llama_index.settings.Settings instead or pass in\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    modules to local functions/methods/interfaces.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceContext is deprecated. Use llama_index.settings.Settings instead, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor pass in modules to local functions/methods/interfaces.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee the docs for updated usage/migration: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: ServiceContext is deprecated. Use llama_index.settings.Settings instead, or pass in modules to local functions/methods/interfaces.\nSee the docs for updated usage/migration: \nhttps://docs.llamaindex.ai/en/stable/module_guides/supporting_modules/service_context_migration/"
     ]
    }
   ],
   "source": [
    "\n",
    "service_context = ServiceContext.from_defaults(llm=gemini_model,embed_model=gemini_embed_model,chunk_size=800,chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
